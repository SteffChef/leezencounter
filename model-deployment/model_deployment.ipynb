{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install packages and import libraries",
   "id": "476f2c13ed681ca8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-06T19:48:33.407541Z",
     "start_time": "2025-07-06T19:48:29.241509Z"
    }
   },
   "source": [
    "# Install required Python packages\n",
    "!pip install --upgrade pip\n",
    "!pip install onnx onnxruntime git+https://github.com/espressif/esp-ppq.git\n",
    "!pip install protobuf==3.20.2\n",
    "!pip install torchvision==0.17.2\n",
    "!git lfs install\n",
    "!find imagenet-sample-images/ -mindepth 1 -name \".*\" -exec rm -rf {} +\n",
    "!pip install ultralytics\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (25.1.1)\r\n",
      "Collecting git+https://github.com/espressif/esp-ppq.git\r\n",
      "  Cloning https://github.com/espressif/esp-ppq.git to /private/var/folders/mv/96db_j8j3ng3t9pc8jczncdh0000gn/T/pip-req-build-ftihjphm\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/espressif/esp-ppq.git /private/var/folders/mv/96db_j8j3ng3t9pc8jczncdh0000gn/T/pip-req-build-ftihjphm\r\n",
      "  Resolved https://github.com/espressif/esp-ppq.git to commit 2d66669c6d264d64d00b677b3c581f4350c3642c\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: onnx in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (1.17.0)\r\n",
      "Requirement already satisfied: onnxruntime in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (1.22.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ppq==0.1.2) (1.26.4)\r\n",
      "Requirement already satisfied: protobuf==3.20.2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ppq==0.1.2) (3.20.2)\r\n",
      "Requirement already satisfied: torch<=2.4.1,>=1.12.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ppq==0.1.2) (2.2.2)\r\n",
      "Requirement already satisfied: onnxsim in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ppq==0.1.2) (0.4.36)\r\n",
      "Requirement already satisfied: tqdm in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ppq==0.1.2) (4.67.1)\r\n",
      "Requirement already satisfied: flatbuffers in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ppq==0.1.2) (25.2.10)\r\n",
      "Requirement already satisfied: cryptography in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ppq==0.1.2) (44.0.3)\r\n",
      "Requirement already satisfied: filelock in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch<=2.4.1,>=1.12.0->ppq==0.1.2) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch<=2.4.1,>=1.12.0->ppq==0.1.2) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch<=2.4.1,>=1.12.0->ppq==0.1.2) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch<=2.4.1,>=1.12.0->ppq==0.1.2) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch<=2.4.1,>=1.12.0->ppq==0.1.2) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch<=2.4.1,>=1.12.0->ppq==0.1.2) (2025.3.2)\r\n",
      "Requirement already satisfied: coloredlogs in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\r\n",
      "Requirement already satisfied: packaging in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from onnxruntime) (25.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from cryptography->ppq==0.1.2) (1.17.1)\r\n",
      "Requirement already satisfied: pycparser in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from cffi>=1.12->cryptography->ppq==0.1.2) (2.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from jinja2->torch<=2.4.1,>=1.12.0->ppq==0.1.2) (3.0.2)\r\n",
      "Requirement already satisfied: rich in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from onnxsim->ppq==0.1.2) (14.0.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from rich->onnxsim->ppq==0.1.2) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from rich->onnxsim->ppq==0.1.2) (2.19.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim->ppq==0.1.2) (0.1.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from sympy->torch<=2.4.1,>=1.12.0->ppq==0.1.2) (1.3.0)\r\n",
      "Requirement already satisfied: protobuf==3.20.2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (3.20.2)\r\n",
      "Requirement already satisfied: torchvision==0.17.2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (0.17.2)\r\n",
      "Requirement already satisfied: numpy in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torchvision==0.17.2) (1.26.4)\r\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torchvision==0.17.2) (2.2.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torchvision==0.17.2) (11.2.1)\r\n",
      "Requirement already satisfied: filelock in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch==2.2.2->torchvision==0.17.2) (2025.3.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision==0.17.2) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchvision==0.17.2) (1.3.0)\r\n",
      "Updated Git hooks.\r\n",
      "Git LFS initialized.\r\n",
      "find: imagenet-sample-images/: No such file or directory\r\n",
      "Requirement already satisfied: ultralytics in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (8.3.130)\r\n",
      "Requirement already satisfied: numpy>=1.23.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (3.10.3)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (4.11.0.86)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (11.2.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (2.32.3)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (1.15.3)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (2.2.2)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (0.17.2)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (4.67.1)\r\n",
      "Requirement already satisfied: psutil in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (7.0.0)\r\n",
      "Requirement already satisfied: py-cpuinfo in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (2.2.3)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (0.13.2)\r\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from ultralytics) (2.0.14)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\r\n",
      "Requirement already satisfied: filelock in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/christophknaden/git/esp-dl-deployment/.venv/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9c8bd184975fa16c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:48:35.859240Z",
     "start_time": "2025-07-06T19:48:33.411095Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Final\n",
    "from PIL import Image\n",
    "from ppq.api import espdl_quantize_onnx\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from ultralytics import YOLO\n",
    "import sys\n",
    "sys.path.append(\"./coco_detect/generate_onnx\")\n",
    "from export_onnx import ESP_YOLO, ESP_Attention, ESP_Detect\n",
    "from ultralytics.nn.modules import Attention, Detect\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ___________ ____        ____  ____  ____ \n",
      "   / ____/ ___// __ \\      / __ \\/ __ \\/ __ \\\n",
      "  / __/  \\__ \\/ /_/ /_____/ /_/ / /_/ / / / /\n",
      " / /___ ___/ / ____/_____/ ____/ ____/ /_/ / \n",
      "/_____//____/_/         /_/   /_/    \\___\\_\\ \n",
      "\n",
      "\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.11 torch-2.2.2 CPU (Apple M3)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001B[34m\u001B[1mPyTorch:\u001B[0m starting from '../models/yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 64, 80, 80), (1, 80, 80, 80), (1, 64, 40, 40), (1, 80, 40, 40), (1, 64, 20, 20), (1, 80, 20, 20)) (5.4 MB)\n",
      "\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m starting export with onnx 1.17.0 opset 13...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m simplifying with onnxsim 0.4.36...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m export success âœ… 0.6s, saved as '../models/yolo11n.onnx' (10.1 MB)\n",
      "\n",
      "Export complete (0.8s)\n",
      "Results saved to \u001B[1m/Users/christophknaden/git/leezencounter/models\u001B[0m\n",
      "Predict:         yolo predict task=detect model=../models/yolo11n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=../models/yolo11n.onnx imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Make predictions with base YOLO model",
   "id": "83fbb221c6d1dfdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:48:40.794902Z",
     "start_time": "2025-07-06T19:48:35.919062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "model_path = \"coco_detect/models/last.pt\"  # Add your model path here\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "torch_model = model.model.eval()\n",
    "\n",
    "IMAGE_PATH: Final[Path] = Path('./calib_images_compressed') # Path to your images directory\n",
    "OUTPUT_DIR: Final[Path] = Path('./preds_calib_images') # Path to save predictions\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "CONF_THRESHOLD = 0.10  # Minimum confidence for a detection to be considered\n",
    "IOU_THRESHOLD = 0.70   # IoU threshold for Non-Maximum Suppression (NMS)\n",
    "MAX_DETECTIONS = 20\n",
    "\n",
    "for img_path in IMAGE_PATH.glob(\"*.jpg\"):\n",
    "    results = model(\n",
    "        img_path,\n",
    "        conf=CONF_THRESHOLD,\n",
    "        iou=IOU_THRESHOLD,\n",
    "        max_det=MAX_DETECTIONS,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Save the annotated image\n",
    "    results[0].save(filename=OUTPUT_DIR / f\"output_{img_path.name}\")\n",
    "\n",
    "    # Convert the results to a pandas DataFrame\n",
    "    pred_df = results[0].to_df()\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    pred_df.to_csv(OUTPUT_DIR / f\"output_{img_path.stem}.csv\", index=False)\n"
   ],
   "id": "7cac35df6ccd5d0c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create .onnx file",
   "id": "b3a0e81655f327b2"
  },
  {
   "cell_type": "code",
   "id": "4c645cfe180034f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:48:41.504081Z",
     "start_time": "2025-07-06T19:48:40.806556Z"
    }
   },
   "source": [
    "model = ESP_YOLO(model_path)\n",
    "for m in model.modules():\n",
    "    if isinstance(m, Attention):\n",
    "        m.forward = ESP_Attention.forward.__get__(m)\n",
    "    if isinstance(m, Detect):\n",
    "        m.forward = ESP_Detect.forward.__get__(m)\n",
    "\n",
    "# Export to ONNX\n",
    "model.export(format=\"onnx\", simplify=True, opset=13, dynamic=True, imgsz=640)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.130 ðŸš€ Python-3.10.11 torch-2.2.2 CPU (Apple M3)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001B[34m\u001B[1mPyTorch:\u001B[0m starting from 'coco_detect/models/last.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 64, 80, 80), (1, 2, 80, 80), (1, 64, 40, 40), (1, 2, 40, 40), (1, 64, 20, 20), (1, 2, 20, 20)) (5.2 MB)\n",
      "\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m starting export with onnx 1.17.0 opset 13...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m simplifying with onnxsim 0.4.36...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m export success âœ… 0.5s, saved as 'coco_detect/models/last.onnx' (9.9 MB)\n",
      "\n",
      "Export complete (0.7s)\n",
      "Results saved to \u001B[1m/Users/christophknaden/git/leezencounter/model-deployment/coco_detect/models\u001B[0m\n",
      "Predict:         yolo predict task=detect model=coco_detect/models/last.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=coco_detect/models/last.onnx imgsz=640 data=./datasets/combined_rotate/combined_yolo_dataset.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'coco_detect/models/last.onnx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create calibration dataset",
   "id": "7564817e582352b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:48:41.516511Z",
     "start_time": "2025-07-06T19:48:41.512599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE: str = 'cpu'\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "IMAGE_SIZE = 640\n",
    "\n",
    "IMAGENET_PATH: Final[Path] = Path('./calib_images_compressed') # Path to your calibration images directory\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, image_dir: Path, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        # Get the list of file names\n",
    "        file_names = [\n",
    "            f for f in os.listdir(image_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "        ]\n",
    "\n",
    "        self.image_files = sorted(file_names)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Now, idx reliably maps to the sorted list of file names\n",
    "        image_path = self.image_dir / self.image_files[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            transformed_image = self.transform(image)\n",
    "\n",
    "        return transformed_image\n",
    "\n",
    "# Transformation (resize only â€” no normalization!)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "cal_dataset = ImageFolderDataset(IMAGENET_PATH, transform=transform)\n",
    "cal_dataloader = DataLoader(\n",
    "    cal_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"--- Mapping of Index to Filename (First 10 Files) ---\")\n",
    "for i in range(len(cal_dataset)):\n",
    "    # We don't need the tensor here, just the filename from the internal list\n",
    "    filename = cal_dataset.image_files[i]\n",
    "    print(f\"Index {i}: {filename}\")"
   ],
   "id": "c8887da4ee902fdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Mapping of Index to Filename (First 10 Files) ---\n",
      "Index 0: frame0.jpg\n",
      "Index 1: frame1.jpg\n",
      "Index 2: frame10.jpg\n",
      "Index 3: frame11.jpg\n",
      "Index 4: frame12.jpg\n",
      "Index 5: frame13.jpg\n",
      "Index 6: frame14.jpg\n",
      "Index 7: frame15.jpg\n",
      "Index 8: frame16.jpg\n",
      "Index 9: frame17.jpg\n",
      "Index 10: frame18.jpg\n",
      "Index 11: frame19.jpg\n",
      "Index 12: frame2.jpg\n",
      "Index 13: frame20.jpg\n",
      "Index 14: frame21.jpg\n",
      "Index 15: frame22.jpg\n",
      "Index 16: frame23.jpg\n",
      "Index 17: frame24.jpg\n",
      "Index 18: frame25.jpg\n",
      "Index 19: frame26.jpg\n",
      "Index 20: frame27.jpg\n",
      "Index 21: frame28.jpg\n",
      "Index 22: frame29.jpg\n",
      "Index 23: frame3.jpg\n",
      "Index 24: frame30.jpg\n",
      "Index 25: frame31.jpg\n",
      "Index 26: frame32.jpg\n",
      "Index 27: frame33.jpg\n",
      "Index 28: frame34.jpg\n",
      "Index 29: frame35.jpg\n",
      "Index 30: frame36.jpg\n",
      "Index 31: frame37.jpg\n",
      "Index 32: frame38.jpg\n",
      "Index 33: frame39.jpg\n",
      "Index 34: frame4.jpg\n",
      "Index 35: frame40.jpg\n",
      "Index 36: frame41.jpg\n",
      "Index 37: frame42.jpg\n",
      "Index 38: frame43.jpg\n",
      "Index 39: frame44.jpg\n",
      "Index 40: frame45.jpg\n",
      "Index 41: frame49.jpg\n",
      "Index 42: frame5.jpg\n",
      "Index 43: frame50.jpg\n",
      "Index 44: frame51.jpg\n",
      "Index 45: frame52.jpg\n",
      "Index 46: frame53.jpg\n",
      "Index 47: frame54.jpg\n",
      "Index 48: frame55.jpg\n",
      "Index 49: frame6.jpg\n",
      "Index 50: frame60.jpg\n",
      "Index 51: frame66.jpg\n",
      "Index 52: frame67.jpg\n",
      "Index 53: frame68.jpg\n",
      "Index 54: frame69.jpg\n",
      "Index 55: frame7.jpg\n",
      "Index 56: frame70.jpg\n",
      "Index 57: frame71.jpg\n",
      "Index 58: frame72.jpg\n",
      "Index 59: frame73.jpg\n",
      "Index 60: frame74.jpg\n",
      "Index 61: frame75.jpg\n",
      "Index 62: frame8.jpg\n",
      "Index 63: frame82.jpg\n",
      "Index 64: frame83.jpg\n",
      "Index 65: frame84.jpg\n",
      "Index 66: frame85.jpg\n",
      "Index 67: frame86.jpg\n",
      "Index 68: frame87.jpg\n",
      "Index 69: frame9.jpg\n",
      "Index 70: frame93.jpg\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create .espdl file",
   "id": "170d6fc10c7437e2"
  },
  {
   "cell_type": "code",
   "id": "a6343c205caf27eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:50:44.888541Z",
     "start_time": "2025-07-06T19:48:41.535034Z"
    }
   },
   "source": [
    "ONNX_YOLO_PATH: Final[Path] = Path('./coco_detect/generate_onnx/last.onnx') # Path to your ONNX model\n",
    "ESPDL_YOLO_PATH: Final[Path] = Path('coco_detect/models/yolo11n.espdl') # Path to save the ESPDL model\n",
    "TARGET_SOC: Final[str] = 'esp32s3'\n",
    "NUM_OF_BITS: Final[int] = 8\n",
    "\n",
    "\n",
    "x = cal_dataset[0]\n",
    "if isinstance(x, (tuple, list)):\n",
    "    x = x[0]\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "def collate_fn(batch: torch.Tensor) -> torch.Tensor:\n",
    "    return batch.to(DEVICE)\n",
    "\n",
    "# make use of ESP-PPQ to quantize the ONNX computation graph, optimize it for ESP32-S3 SoC, and convert it to .espdl\n",
    "quantized_model = espdl_quantize_onnx(\n",
    "    onnx_import_file=ONNX_YOLO_PATH.as_posix(),\n",
    "    espdl_export_file=ESPDL_YOLO_PATH.as_posix(),\n",
    "    calib_dataloader=cal_dataloader,\n",
    "    calib_steps=8,\n",
    "    input_shape=x.shape,\n",
    "    inputs=None,\n",
    "    target=TARGET_SOC,\n",
    "    num_of_bits=NUM_OF_BITS,\n",
    "    collate_fn=collate_fn,\n",
    "    dispatching_override=None,\n",
    "    device=DEVICE,\n",
    "    error_report=True,\n",
    "    skip_export=False,\n",
    "    export_test_values=True,\n",
    "    verbose=1,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31m[Warning] Unexpected input value of operation /model.11/Resize, recieving \"None\" at its input 1\u001B[0m\n",
      "\u001B[31m[Warning] Unexpected input value of operation /model.14/Resize, recieving \"None\" at its input 1\u001B[0m\n",
      "[21:48:42] PPQ Quantization Fusion Pass Running ...       Finished.\n",
      "[21:48:42] PPQ Quantize Simplify Pass Running ...         Finished.\n",
      "[21:48:42] PPQ Parameter Quantization Pass Running ...    Finished.\n",
      "[21:48:42] PPQ Runtime Calibration Pass Running ...       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.72it/s]\n",
      "Calibration Progress(Phase 2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "[21:48:44] PPQ Quantization Alignment Pass Running ...    Finished.\n",
      "[21:48:44] PPQ Passive Parameter Quantization Running ... Finished.\n",
      "--------- Network Snapshot ---------\n",
      "Num of Op:                    [296]\n",
      "Num of Quantized Op:          [296]\n",
      "Num of Variable:              [499]\n",
      "Num of Quantized Var:         [499]\n",
      "------- Quantization Snapshot ------\n",
      "Num of Quant Config:          [915]\n",
      "ACTIVATED:                    [334]\n",
      "OVERLAPPED:                   [369]\n",
      "PASSIVE:                      [194]\n",
      "FP32:                         [18]\n",
      "Network Quantization Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysing Graphwise Quantization Error(Phrase 1):: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  6.58it/s]\n",
      "Analysing Graphwise Quantization Error(Phrase 2):: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer                                        | NOISE:SIGNAL POWER RATIO \n",
      "/model.10/m/m.0/ffn/ffn.1/conv/Conv:         | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 17.327%\n",
      "/model.10/m/m.0/attn/proj/conv/Conv:         | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 16.930%\n",
      "/model.22/m.0/cv2/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 13.361%\n",
      "/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 13.015%\n",
      "/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 12.839%\n",
      "/model.23/cv3.2/cv3.2.1/cv3.2.1.1/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 12.043%\n",
      "/model.10/m/m.0/attn/MatMul_1:               | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 10.916%\n",
      "/model.23/cv3.2/cv3.2.1/cv3.2.1.0/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 10.662%\n",
      "/model.10/m/m.0/attn/MatMul:                 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 10.419%\n",
      "/model.23/cv3.1/cv3.1.1/cv3.1.1.1/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 10.208%\n",
      "/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 10.111%\n",
      "/model.23/cv2.0/cv2.0.1/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 10.013%\n",
      "/model.23/cv3.0/cv3.0.0/cv3.0.0.1/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 9.923%\n",
      "/model.10/m/m.0/attn/qkv/conv/Conv:          | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 9.814%\n",
      "/model.23/cv3.2/cv3.2.0/cv3.2.0.1/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 8.982%\n",
      "/model.22/m.0/cv3/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 8.608%\n",
      "/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 8.546%\n",
      "/model.10/m/m.0/attn/pe/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 8.526%\n",
      "/model.10/cv1/conv/Conv:                     | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 7.698%\n",
      "/model.23/cv3.2/cv3.2.0/cv3.2.0.0/conv/Conv: | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 7.358%\n",
      "/model.22/m.0/m/m.0/cv2/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 6.966%\n",
      "/model.22/cv2/conv/Conv:                     | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 6.714%\n",
      "/model.23/cv2.2/cv2.2.0/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 6.453%\n",
      "/model.22/cv1/conv/Conv:                     | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 6.422%\n",
      "/model.8/m.0/cv2/conv/Conv:                  | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 6.390%\n",
      "/model.22/m.0/m/m.1/cv2/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 6.167%\n",
      "/model.23/cv2.1/cv2.1.1/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 5.681%\n",
      "/model.23/cv2.2/cv2.2.1/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 5.628%\n",
      "/model.8/m.0/m/m.1/cv2/conv/Conv:            | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 5.550%\n",
      "/model.19/m.0/cv2/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 5.481%\n",
      "/model.23/cv2.1/cv2.1.0/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 5.135%\n",
      "/model.22/m.0/cv1/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 5.103%\n",
      "/model.22/m.0/m/m.0/cv1/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 5.042%\n",
      "/model.22/m.0/m/m.1/cv1/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4.578%\n",
      "/model.8/cv2/conv/Conv:                      | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4.330%\n",
      "/model.8/m.0/cv3/conv/Conv:                  | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4.169%\n",
      "/model.10/cv2/conv/Conv:                     | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4.133%\n",
      "/model.8/m.0/cv1/conv/Conv:                  | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4.109%\n",
      "/model.8/cv1/conv/Conv:                      | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4.092%\n",
      "/model.13/m.0/cv2/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4.040%\n",
      "/model.20/conv/Conv:                         | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.984%\n",
      "/model.16/m.0/cv2/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.928%\n",
      "/model.6/m.0/cv2/conv/Conv:                  | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.903%\n",
      "/model.19/cv1/conv/Conv:                     | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.644%\n",
      "/model.19/m.0/cv1/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.617%\n",
      "/model.16/m.0/cv1/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.593%\n",
      "/model.19/cv2/conv/Conv:                     | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.463%\n",
      "/model.8/m.0/m/m.0/cv2/conv/Conv:            | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.416%\n",
      "/model.13/m.0/cv1/conv/Conv:                 | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.387%\n",
      "/model.13/cv2/conv/Conv:                     | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.344%\n",
      "/model.23/cv2.0/cv2.0.0/conv/Conv:           | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.277%\n",
      "/model.8/m.0/m/m.0/cv1/conv/Conv:            | â–ˆâ–ˆâ–ˆâ–ˆ                 | 3.224%\n",
      "/model.23/cv2.0/cv2.0.2/Conv:                | â–ˆâ–ˆâ–ˆ                  | 3.155%\n",
      "/model.8/m.0/m/m.1/cv1/conv/Conv:            | â–ˆâ–ˆâ–ˆ                  | 3.130%\n",
      "/model.17/conv/Conv:                         | â–ˆâ–ˆâ–ˆ                  | 3.028%\n",
      "/model.13/cv1/conv/Conv:                     | â–ˆâ–ˆâ–ˆ                  | 3.024%\n",
      "/model.6/cv1/conv/Conv:                      | â–ˆâ–ˆâ–ˆ                  | 2.972%\n",
      "/model.6/cv2/conv/Conv:                      | â–ˆâ–ˆâ–ˆ                  | 2.852%\n",
      "/model.10/m/m.0/ffn/ffn.0/conv/Conv:         | â–ˆâ–ˆâ–ˆ                  | 2.833%\n",
      "/model.6/m.0/cv3/conv/Conv:                  | â–ˆâ–ˆâ–ˆ                  | 2.758%\n",
      "/model.9/cv2/conv/Conv:                      | â–ˆâ–ˆâ–ˆ                  | 2.542%\n",
      "/model.16/cv2/conv/Conv:                     | â–ˆâ–ˆâ–ˆ                  | 2.426%\n",
      "/model.2/m.0/cv2/conv/Conv:                  | â–ˆâ–ˆ                   | 2.277%\n",
      "/model.6/m.0/m/m.0/cv2/conv/Conv:            | â–ˆâ–ˆ                   | 2.273%\n",
      "/model.4/cv2/conv/Conv:                      | â–ˆâ–ˆ                   | 2.219%\n",
      "/model.7/conv/Conv:                          | â–ˆâ–ˆ                   | 2.188%\n",
      "/model.4/cv1/conv/Conv:                      | â–ˆâ–ˆ                   | 2.148%\n",
      "/model.3/conv/Conv:                          | â–ˆâ–ˆ                   | 2.127%\n",
      "/model.6/m.0/cv1/conv/Conv:                  | â–ˆâ–ˆ                   | 2.064%\n",
      "/model.5/conv/Conv:                          | â–ˆâ–ˆ                   | 2.021%\n",
      "/model.6/m.0/m/m.1/cv1/conv/Conv:            | â–ˆâ–ˆ                   | 2.003%\n",
      "/model.23/cv2.2/cv2.2.2/Conv:                | â–ˆâ–ˆ                   | 1.984%\n",
      "/model.6/m.0/m/m.0/cv1/conv/Conv:            | â–ˆâ–ˆ                   | 1.951%\n",
      "/model.23/cv2.1/cv2.1.2/Conv:                | â–ˆâ–ˆ                   | 1.842%\n",
      "/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv: | â–ˆâ–ˆ                   | 1.669%\n",
      "/model.4/m.0/cv1/conv/Conv:                  | â–ˆâ–ˆ                   | 1.610%\n",
      "/model.6/m.0/m/m.1/cv2/conv/Conv:            | â–ˆâ–ˆ                   | 1.508%\n",
      "/model.16/cv1/conv/Conv:                     | â–ˆâ–ˆ                   | 1.504%\n",
      "/model.2/cv2/conv/Conv:                      | â–ˆ                    | 1.420%\n",
      "/model.4/m.0/cv2/conv/Conv:                  | â–ˆ                    | 1.111%\n",
      "/model.9/cv1/conv/Conv:                      | â–ˆ                    | 0.929%\n",
      "/model.2/cv1/conv/Conv:                      | â–ˆ                    | 0.781%\n",
      "/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv: | â–ˆ                    | 0.656%\n",
      "/model.23/cv3.0/cv3.0.2/Conv:                |                      | 0.437%\n",
      "/model.2/m.0/cv1/conv/Conv:                  |                      | 0.413%\n",
      "/model.1/conv/Conv:                          |                      | 0.336%\n",
      "/model.23/cv3.1/cv3.1.2/Conv:                |                      | 0.263%\n",
      "/model.0/conv/Conv:                          |                      | 0.169%\n",
      "/model.23/cv3.2/cv3.2.2/Conv:                |                      | 0.154%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysing Layerwise quantization error:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [01:44<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer                                        | NOISE:SIGNAL POWER RATIO \n",
      "/model.9/cv2/conv/Conv:                      | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 0.117%\n",
      "/model.4/cv1/conv/Conv:                      | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 0.064%\n",
      "/model.4/cv2/conv/Conv:                      | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 0.063%\n",
      "/model.23/cv3.0/cv3.0.2/Conv:                | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 0.063%\n",
      "/model.1/conv/Conv:                          | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 0.052%\n",
      "/model.0/conv/Conv:                          | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 0.050%\n",
      "/model.2/cv1/conv/Conv:                      | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 0.036%\n",
      "/model.23/cv3.1/cv3.1.2/Conv:                | â–ˆâ–ˆâ–ˆâ–ˆ                 | 0.024%\n",
      "/model.8/cv2/conv/Conv:                      | â–ˆâ–ˆâ–ˆ                  | 0.017%\n",
      "/model.3/conv/Conv:                          | â–ˆâ–ˆâ–ˆ                  | 0.016%\n",
      "/model.16/m.0/cv2/conv/Conv:                 | â–ˆâ–ˆâ–ˆ                  | 0.015%\n",
      "/model.16/cv2/conv/Conv:                     | â–ˆâ–ˆ                   | 0.013%\n",
      "/model.2/m.0/cv1/conv/Conv:                  | â–ˆâ–ˆ                   | 0.012%\n",
      "/model.16/m.0/cv1/conv/Conv:                 | â–ˆâ–ˆ                   | 0.012%\n",
      "/model.2/cv2/conv/Conv:                      | â–ˆâ–ˆ                   | 0.011%\n",
      "/model.16/cv1/conv/Conv:                     | â–ˆâ–ˆ                   | 0.011%\n",
      "/model.4/m.0/cv1/conv/Conv:                  | â–ˆâ–ˆ                   | 0.010%\n",
      "/model.22/cv1/conv/Conv:                     | â–ˆâ–ˆ                   | 0.010%\n",
      "/model.19/m.0/cv1/conv/Conv:                 | â–ˆ                    | 0.008%\n",
      "/model.13/cv2/conv/Conv:                     | â–ˆ                    | 0.008%\n",
      "/model.2/m.0/cv2/conv/Conv:                  | â–ˆ                    | 0.008%\n",
      "/model.10/cv1/conv/Conv:                     | â–ˆ                    | 0.008%\n",
      "/model.4/m.0/cv2/conv/Conv:                  | â–ˆ                    | 0.008%\n",
      "/model.13/cv1/conv/Conv:                     | â–ˆ                    | 0.007%\n",
      "/model.6/cv1/conv/Conv:                      | â–ˆ                    | 0.006%\n",
      "/model.23/cv2.0/cv2.0.0/conv/Conv:           | â–ˆ                    | 0.006%\n",
      "/model.13/m.0/cv1/conv/Conv:                 | â–ˆ                    | 0.006%\n",
      "/model.23/cv2.2/cv2.2.2/Conv:                | â–ˆ                    | 0.004%\n",
      "/model.23/cv2.0/cv2.0.2/Conv:                | â–ˆ                    | 0.004%\n",
      "/model.23/cv2.1/cv2.1.2/Conv:                | â–ˆ                    | 0.004%\n",
      "/model.23/cv2.1/cv2.1.1/conv/Conv:           | â–ˆ                    | 0.004%\n",
      "/model.23/cv2.0/cv2.0.1/conv/Conv:           | â–ˆ                    | 0.003%\n",
      "/model.23/cv2.2/cv2.2.1/conv/Conv:           | â–ˆ                    | 0.003%\n",
      "/model.19/m.0/cv2/conv/Conv:                 |                      | 0.003%\n",
      "/model.23/cv2.1/cv2.1.0/conv/Conv:           |                      | 0.003%\n",
      "/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv: |                      | 0.003%\n",
      "/model.5/conv/Conv:                          |                      | 0.002%\n",
      "/model.10/cv2/conv/Conv:                     |                      | 0.002%\n",
      "/model.6/cv2/conv/Conv:                      |                      | 0.002%\n",
      "/model.19/cv1/conv/Conv:                     |                      | 0.002%\n",
      "/model.6/m.0/cv3/conv/Conv:                  |                      | 0.002%\n",
      "/model.10/m/m.0/attn/pe/conv/Conv:           |                      | 0.002%\n",
      "/model.9/cv1/conv/Conv:                      |                      | 0.002%\n",
      "/model.10/m/m.0/attn/qkv/conv/Conv:          |                      | 0.002%\n",
      "/model.19/cv2/conv/Conv:                     |                      | 0.001%\n",
      "/model.17/conv/Conv:                         |                      | 0.001%\n",
      "/model.8/cv1/conv/Conv:                      |                      | 0.001%\n",
      "/model.22/cv2/conv/Conv:                     |                      | 0.001%\n",
      "/model.23/cv2.2/cv2.2.0/conv/Conv:           |                      | 0.001%\n",
      "/model.13/m.0/cv2/conv/Conv:                 |                      | 0.001%\n",
      "/model.10/m/m.0/attn/MatMul:                 |                      | 0.001%\n",
      "/model.10/m/m.0/attn/MatMul_1:               |                      | 0.001%\n",
      "/model.22/m.0/m/m.1/cv1/conv/Conv:           |                      | 0.001%\n",
      "/model.22/m.0/m/m.0/cv1/conv/Conv:           |                      | 0.001%\n",
      "/model.10/m/m.0/attn/proj/conv/Conv:         |                      | 0.001%\n",
      "/model.22/m.0/m/m.1/cv2/conv/Conv:           |                      | 0.001%\n",
      "/model.6/m.0/m/m.0/cv1/conv/Conv:            |                      | 0.001%\n",
      "/model.6/m.0/m/m.1/cv2/conv/Conv:            |                      | 0.000%\n",
      "/model.8/m.0/cv1/conv/Conv:                  |                      | 0.000%\n",
      "/model.8/m.0/cv3/conv/Conv:                  |                      | 0.000%\n",
      "/model.7/conv/Conv:                          |                      | 0.000%\n",
      "/model.22/m.0/m/m.0/cv2/conv/Conv:           |                      | 0.000%\n",
      "/model.6/m.0/m/m.0/cv2/conv/Conv:            |                      | 0.000%\n",
      "/model.22/m.0/cv1/conv/Conv:                 |                      | 0.000%\n",
      "/model.23/cv3.2/cv3.2.2/Conv:                |                      | 0.000%\n",
      "/model.10/m/m.0/ffn/ffn.1/conv/Conv:         |                      | 0.000%\n",
      "/model.6/m.0/m/m.1/cv1/conv/Conv:            |                      | 0.000%\n",
      "/model.6/m.0/cv1/conv/Conv:                  |                      | 0.000%\n",
      "/model.22/m.0/cv3/conv/Conv:                 |                      | 0.000%\n",
      "/model.20/conv/Conv:                         |                      | 0.000%\n",
      "/model.23/cv3.2/cv3.2.0/cv3.2.0.0/conv/Conv: |                      | 0.000%\n",
      "/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv: |                      | 0.000%\n",
      "/model.8/m.0/m/m.0/cv2/conv/Conv:            |                      | 0.000%\n",
      "/model.8/m.0/m/m.1/cv2/conv/Conv:            |                      | 0.000%\n",
      "/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv: |                      | 0.000%\n",
      "/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv: |                      | 0.000%\n",
      "/model.8/m.0/m/m.1/cv1/conv/Conv:            |                      | 0.000%\n",
      "/model.23/cv3.2/cv3.2.1/cv3.2.1.0/conv/Conv: |                      | 0.000%\n",
      "/model.8/m.0/m/m.0/cv1/conv/Conv:            |                      | 0.000%\n",
      "/model.23/cv3.2/cv3.2.0/cv3.2.0.1/conv/Conv: |                      | 0.000%\n",
      "/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv: |                      | 0.000%\n",
      "/model.23/cv3.0/cv3.0.0/cv3.0.0.1/conv/Conv: |                      | 0.000%\n",
      "/model.10/m/m.0/ffn/ffn.0/conv/Conv:         |                      | 0.000%\n",
      "/model.6/m.0/cv2/conv/Conv:                  |                      | 0.000%\n",
      "/model.23/cv3.2/cv3.2.1/cv3.2.1.1/conv/Conv: |                      | 0.000%\n",
      "/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv: |                      | 0.000%\n",
      "/model.23/cv3.1/cv3.1.1/cv3.1.1.1/conv/Conv: |                      | 0.000%\n",
      "/model.22/m.0/cv2/conv/Conv:                 |                      | 0.000%\n",
      "/model.8/m.0/cv2/conv/Conv:                  |                      | 0.000%\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_185 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_8 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_5 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_2 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_3 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Concat_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_332 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_11 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Softmax_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_10 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_6 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_1 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_9 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_7 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_4 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_185 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_8 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_5 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_2 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_3 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Concat_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_332 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_11 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Softmax_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_10 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_6 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_1 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_9 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_7 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_4 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_185 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_8 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_5 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_2 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_3 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Concat_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_332 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_11 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Softmax_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_10 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_6 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_1 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_9 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_7 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_4 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_185 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_8 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_5 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_2 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_3 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Concat_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip onnx::Split_332 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_64 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_11 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_67 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip /model.10/m/m.0/attn/Softmax_output_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_69 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_72 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_10 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_0 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_6 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip  because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_1 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_9 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_7 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mSkip PPQ_Variable_4 because it's not exportable\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;2m[INFO][ESPDL][2025-07-06 21:50:32]:  \u001B[mskip not QuantableOperation\n",
      "\u001B[38;5;3m[WARNING][ESPDL][2025-07-06 21:50:33]:  \u001B[m/model.10/m/m.0/attn/Softmax_output_0 does not bind exponents parameter\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Image Preprocessing for inference",
   "id": "157f9f9cc91399e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pre and Post-processing for ESP-DL",
   "id": "5f2d59f4acd72c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:50:46.735525Z",
     "start_time": "2025-07-06T19:50:46.728044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from ppq import TorchExecutor\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_for_esp_dl(image_path, model_input_shape, mean, std):\n",
    "    \"\"\" Replicates the preprocessing logic from the ESP-DL C++ code. \"\"\"\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    assert img_bgr is not None, f\"Image not found at {image_path}\"\n",
    "    original_h, original_w = img_bgr.shape[:2]\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    target_h, target_w = model_input_shape\n",
    "    resized_img = cv2.resize(img_rgb, (target_w, target_h), interpolation=cv2.INTER_NEAREST)\n",
    "    img_tensor = torch.from_numpy(resized_img).float()\n",
    "    mean_tensor = torch.tensor(mean, dtype=torch.float32).reshape(1, 1, 3)\n",
    "    std_tensor = torch.tensor(std, dtype=torch.float32).reshape(1, 1, 3)\n",
    "    normalized_tensor = (img_tensor - mean_tensor) / std_tensor\n",
    "    input_tensor = normalized_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "    resize_scale_x = target_w / original_w\n",
    "    resize_scale_y = target_h / original_h\n",
    "    return input_tensor, (original_w, original_h), (resize_scale_x, resize_scale_y)\n",
    "\n",
    "\n",
    "def postprocess_for_esp_dl(outputs, original_shape, resize_scales,\n",
    "                           conf_threshold, iou_threshold, max_detections):\n",
    "    \"\"\" Replicates the post-processing logic from the ESP-DL C++ code. \"\"\"\n",
    "    strides = [8, 16, 32]\n",
    "    reg_max = 16\n",
    "    bins = torch.arange(reg_max, device=outputs[0].device, dtype=torch.float32)\n",
    "    all_boxes, all_scores, all_class_ids = [], [], []\n",
    "\n",
    "    box_preds = [outputs[0], outputs[2], outputs[4]]\n",
    "    cls_preds = [outputs[1], outputs[3], outputs[5]]\n",
    "\n",
    "    for i in range(len(strides)):\n",
    "        box_pred, cls_pred = box_preds[i], cls_preds[i]\n",
    "        stride = strides[i]\n",
    "        height, width = cls_pred.shape[2], cls_pred.shape[3]\n",
    "\n",
    "        cls_pred = cls_pred.permute(0, 2, 3, 1).reshape(1, -1, cls_pred.shape[1])[0]\n",
    "        box_pred = box_pred.permute(0, 2, 3, 1).reshape(1, -1, 4 * reg_max)[0]\n",
    "\n",
    "        scores, class_ids = torch.sigmoid(cls_pred).max(1)\n",
    "        confident_mask = scores > conf_threshold\n",
    "        if not confident_mask.any():\n",
    "            continue\n",
    "\n",
    "        confident_boxes = box_pred[confident_mask]\n",
    "        confident_scores = scores[confident_mask]\n",
    "        confident_class_ids = class_ids[confident_mask]\n",
    "\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(height, device=box_pred.device, dtype=torch.float32),\n",
    "                                        torch.arange(width, device=box_pred.device, dtype=torch.float32),\n",
    "                                        indexing='ij')\n",
    "        grid_coords_centered = torch.stack((grid_x.flatten() + 0.5, grid_y.flatten() + 0.5), dim=1)\n",
    "        confident_grid_coords = grid_coords_centered[confident_mask]\n",
    "\n",
    "        box_pred_probs = torch.softmax(confident_boxes.reshape(-1, reg_max), dim=-1)\n",
    "        box_reg_dist = torch.matmul(box_pred_probs, bins).reshape(-1, 4)\n",
    "        d_left, d_top, d_right, d_bottom = box_reg_dist.chunk(4, dim=1)\n",
    "\n",
    "        x1 = (confident_grid_coords[:, 0] - d_left.squeeze(-1)) * stride\n",
    "        y1 = (confident_grid_coords[:, 1] - d_top.squeeze(-1)) * stride\n",
    "        x2 = (confident_grid_coords[:, 0] + d_right.squeeze(-1)) * stride\n",
    "        y2 = (confident_grid_coords[:, 1] + d_bottom.squeeze(-1)) * stride\n",
    "        decoded_boxes = torch.stack((x1, y1, x2, y2), dim=1)\n",
    "\n",
    "        all_boxes.append(decoded_boxes)\n",
    "        all_scores.append(confident_scores)\n",
    "        all_class_ids.append(confident_class_ids)\n",
    "\n",
    "    if not all_boxes:\n",
    "        return []\n",
    "\n",
    "    final_boxes = torch.cat(all_boxes, dim=0)\n",
    "    final_scores = torch.cat(all_scores, dim=0)\n",
    "    final_class_ids = torch.cat(all_class_ids, dim=0)\n",
    "\n",
    "    inv_resize_scale_x = 1.0 / resize_scales[0]\n",
    "    inv_resize_scale_y = 1.0 / resize_scales[1]\n",
    "    final_boxes[:, [0, 2]] *= inv_resize_scale_x\n",
    "    final_boxes[:, [1, 3]] *= inv_resize_scale_y\n",
    "\n",
    "    orig_w, orig_h = original_shape\n",
    "    final_boxes[:, 0].clamp_(0, orig_w)\n",
    "    final_boxes[:, 1].clamp_(0, orig_h)\n",
    "    final_boxes[:, 2].clamp_(0, orig_w)\n",
    "    final_boxes[:, 3].clamp_(0, orig_h)\n",
    "\n",
    "    nms_indices = torchvision.ops.nms(final_boxes, final_scores, iou_threshold)\n",
    "    if len(nms_indices) > max_detections:\n",
    "        nms_indices = nms_indices[:max_detections]\n",
    "\n",
    "    detections = []\n",
    "    for i in nms_indices:\n",
    "        box = final_boxes[i].cpu().numpy().astype(int)\n",
    "        score = final_scores[i].cpu().item()\n",
    "        class_id = final_class_ids[i].cpu().item()\n",
    "        detections.append( (class_id, score, box[0], box[1], box[2], box[3]) )\n",
    "\n",
    "    # Sort by score descending to match the C++ list behavior\n",
    "    detections.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return detections\n"
   ],
   "id": "9e78e9c3ae8ca6e3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation of predictions over all images compared to base model (without quantization)",
   "id": "79c4070fed03b15c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:51:02.738119Z",
     "start_time": "2025-07-06T19:50:46.739595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from ppq import TorchExecutor\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "def load_ground_truth_from_csv(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except EmptyDataError:\n",
    "        return [], []\n",
    "    gt_boxes = []\n",
    "    gt_class_ids = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        box_dict = ast.literal_eval(row['box'])\n",
    "        box = [box_dict['x1'], box_dict['y1'], box_dict['x2'], box_dict['y2']]\n",
    "        gt_boxes.append(box)\n",
    "        gt_class_ids.append(row['class'])\n",
    "\n",
    "    return torch.tensor(gt_boxes, dtype=torch.float32), torch.tensor(gt_class_ids, dtype=torch.int64)\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x_left = max(box1[0], box2[0])\n",
    "    y_top = max(box1[1], box2[1])\n",
    "    x_right = min(box1[2], box2[2])\n",
    "    y_bottom = min(box1[3], box2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_ap_for_class_across_images(predictions, ground_truths_by_image, total_gt_count, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates AP and also returns the IoU scores of all true positive matches.\n",
    "    \"\"\"\n",
    "    # --- THIS IS THE FIX ---\n",
    "    if total_gt_count == 0:\n",
    "        # If there are no ground truths, AP is 1.0 if there were also no predictions,\n",
    "        # and 0.0 otherwise. In either case, there are no True Positive IoUs.\n",
    "        # We must return a tuple of (float, list) to match the other return path.\n",
    "        fp_count = len(predictions)\n",
    "        ap = 1.0 if not predictions else 0.0\n",
    "        return ap, [], 0, fp_count, 0\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    # The rest of the function remains exactly the same...\n",
    "    predictions.sort(key=lambda x: x[2], reverse=True)\n",
    "    tp = np.zeros(len(predictions))\n",
    "    fp = np.zeros(len(predictions))\n",
    "    gt_used_map = {img_idx: [False] * len(boxes) for img_idx, boxes in ground_truths_by_image.items()}\n",
    "    true_positive_ious = []\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        img_idx, pred_box, score = pred\n",
    "        gt_boxes_for_image = ground_truths_by_image.get(img_idx, [])\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        for j, gt_box in enumerate(gt_boxes_for_image):\n",
    "            iou = calculate_iou(torch.tensor(pred_box), torch.tensor(gt_box))\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = j\n",
    "        if best_iou >= iou_threshold and not gt_used_map[img_idx][best_gt_idx]:\n",
    "            tp[i] = 1\n",
    "            gt_used_map[img_idx][best_gt_idx] = True\n",
    "            true_positive_ious.append(best_iou)\n",
    "        else:\n",
    "            fp[i] = 1\n",
    "\n",
    "    tp_cumsum = np.cumsum(tp)\n",
    "    fp_cumsum = np.cumsum(fp)\n",
    "    recalls = tp_cumsum / total_gt_count\n",
    "    precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "    precisions = np.concatenate(([0.], precisions, [0.]))\n",
    "    recalls = np.concatenate(([0.], recalls, [1.]))\n",
    "    for i in range(len(precisions) - 2, -1, -1):\n",
    "        precisions[i] = max(precisions[i], precisions[i+1])\n",
    "    ap = 0\n",
    "    for i in range(len(recalls) - 1):\n",
    "        ap += (recalls[i+1] - recalls[i]) * precisions[i+1]\n",
    "\n",
    "\n",
    "    tp_count = int(np.sum(tp))\n",
    "    fp_count = int(np.sum(fp))\n",
    "    fn_count = total_gt_count - tp_count\n",
    "    return ap, true_positive_ious, tp_count, fp_count, fn_count\n",
    "\n",
    "def save_predictions_to_csv(detections_tensor, class_names_map, output_csv_path):\n",
    "    columns = ['name', 'class', 'confidence', 'box']\n",
    "\n",
    "    # If there are no detections, create an empty CSV with the correct headers\n",
    "    if detections_tensor is None or detections_tensor.numel() == 0:\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        return\n",
    "\n",
    "    # Create a list of dictionaries, where each dict is a row\n",
    "    rows = []\n",
    "    for det in detections_tensor:\n",
    "        x1, y1, x2, y2, score, class_id_tensor = det\n",
    "\n",
    "        class_id = int(class_id_tensor.item())\n",
    "        confidence = score.item()\n",
    "\n",
    "        # Get the human-readable class name, provide a default if not found\n",
    "        class_name = class_names_map.get(class_id, f\"class_{class_id}\")\n",
    "\n",
    "        # Format the bounding box into the required string format\n",
    "        box_string = f\"{{'x1': {x1.item()}, 'y1': {y1.item()}, 'x2': {x2.item()}, 'y2': {y2.item()}}}\"\n",
    "\n",
    "        rows.append({\n",
    "            'name': class_name,\n",
    "            'class': class_id,\n",
    "            'confidence': confidence,\n",
    "            'box': box_string\n",
    "        })\n",
    "\n",
    "    # Create and save the DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "# --- NEW: Function to evaluate pre-computed CSV predictions ---\n",
    "def evaluate_csv_predictions(image_paths, gt_dir, prediction_dir, class_names_map):\n",
    "    \"\"\"\n",
    "    Evaluates a model's performance based on pre-existing prediction CSV files.\n",
    "\n",
    "    Args:\n",
    "        image_paths: A list of paths to the input images.\n",
    "        gt_dir: The directory containing the ground truth CSV files.\n",
    "        prediction_dir: The directory containing the pre-computed prediction CSV files.\n",
    "        class_names_map: A dictionary mapping class IDs to names.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the evaluation results.\n",
    "    \"\"\"\n",
    "    all_predictions = defaultdict(list)\n",
    "    all_ground_truths = defaultdict(list)\n",
    "\n",
    "    print(f\"\\nEvaluating pre-computed CSVs from '{prediction_dir}'...\")\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        image_name = os.path.basename(image_path)\n",
    "        image_base_name = os.path.splitext(image_name)[0]\n",
    "\n",
    "        # 1. Load Ground Truth\n",
    "        gt_csv_name = f\"output_{image_base_name}.csv\"\n",
    "        gt_csv_path = os.path.join(gt_dir, gt_csv_name)\n",
    "        gt_boxes, gt_classes = load_ground_truth_from_csv(gt_csv_path)\n",
    "\n",
    "        for box, cls_id in zip(gt_boxes, gt_classes):\n",
    "            all_ground_truths[cls_id.item()].append([i, box.tolist()])\n",
    "\n",
    "        # 2. Load Pre-computed Predictions\n",
    "        pred_csv_name = f\"output_{image_base_name}.csv\"\n",
    "        pred_csv_path = os.path.join(prediction_dir, pred_csv_name)\n",
    "        pred_boxes, pred_classes, pred_scores = load_predictions_from_csv(pred_csv_path)\n",
    "\n",
    "        for box, cls_id, score in zip(pred_boxes, pred_classes, pred_scores):\n",
    "            all_predictions[cls_id.item()].append([i, box.tolist(), score.item()])\n",
    "\n",
    "    # --- Calculate metrics (same logic as before) ---\n",
    "    return calculate_metrics_from_collected_data(all_predictions, all_ground_truths)\n",
    "\n",
    "\n",
    "# --- NEW: Function to load predictions from your specific CSV format ---\n",
    "def load_predictions_from_csv(csv_path):\n",
    "    \"\"\"Parses prediction CSVs, returning boxes, classes, and scores.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return [], [], []\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except EmptyDataError:\n",
    "        return [], [], []\n",
    "\n",
    "    pred_boxes = []\n",
    "    pred_class_ids = []\n",
    "    pred_scores = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        box_dict = ast.literal_eval(row['box'])\n",
    "        box = [box_dict['x1'], box_dict['y1'], box_dict['x2'], box_dict['y2']]\n",
    "        pred_boxes.append(box)\n",
    "        pred_class_ids.append(row['class'])\n",
    "        pred_scores.append(row['confidence'])\n",
    "\n",
    "    return torch.tensor(pred_boxes), torch.tensor(pred_class_ids), torch.tensor(pred_scores)\n",
    "\n",
    "\n",
    "# --- NEW: Helper function to avoid code duplication in metric calculation ---\n",
    "def calculate_metrics_from_collected_data(all_predictions, all_ground_truths):\n",
    "    \"\"\"Calculates AP, mAP, and Avg IoU from collected prediction and GT data.\"\"\"\n",
    "    results = {\n",
    "        \"ap_per_class\": {},\n",
    "        \"avg_iou_per_class\": {},\n",
    "        \"tps_per_class\": {},\n",
    "        \"fps_per_class\": {},\n",
    "        \"fns_per_class\": {}\n",
    "    }\n",
    "    all_class_ids = sorted(list(set(all_predictions.keys()) | set(all_ground_truths.keys())))\n",
    "\n",
    "    for class_id in all_class_ids:\n",
    "        class_preds = all_predictions[class_id]\n",
    "        class_gts_by_image = defaultdict(list)\n",
    "        total_gt_count = 0\n",
    "        for img_idx, box_list in all_ground_truths.get(class_id, []):\n",
    "            class_gts_by_image[img_idx].append(box_list)\n",
    "            total_gt_count += 1\n",
    "\n",
    "        ap, tp_ious, tp_count, fp_count, fn_count = calculate_ap_for_class_across_images(\n",
    "            class_preds, class_gts_by_image, total_gt_count, iou_threshold=0.5\n",
    "        )\n",
    "        results[\"ap_per_class\"][class_id] = ap\n",
    "        results[\"avg_iou_per_class\"][class_id] = np.mean(tp_ious) if tp_ious else 0.0\n",
    "        results[\"tps_per_class\"][class_id] = tp_count\n",
    "        results[\"fps_per_class\"][class_id] = fp_count\n",
    "        results[\"fns_per_class\"][class_id] = fn_count\n",
    "\n",
    "    results[\"mAP\"] = np.mean(list(results[\"ap_per_class\"].values())) if results[\"ap_per_class\"] else 0.0\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Main controller script ---\n",
    "def main():\n",
    "    # --- Global Configuration ---\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    IMAGE_DIR = \"./calib_images_compressed\"\n",
    "    GT_DIR = \"./new_ground_truth_labels\"\n",
    "    ORIGINAL_MODEL_PRED_DIR = \"./preds_calib_images\"\n",
    "    QUANTIZED_MODEL_PRED_DIR = \"./preds_quantized_model\"\n",
    "    CLASS_NAMES = {0: 'bicycle', 1: 'saddle'} # Update this!\n",
    "    MODEL_MEAN = [0, 0, 0]\n",
    "    MODEL_STD = [255, 255, 255]\n",
    "    MODEL_INPUT_SHAPE = (640, 640)\n",
    "    CONF_THRESHOLD = 0.10\n",
    "    IOU_THRESHOLD = 0.70\n",
    "    MAX_DETECTIONS = 20\n",
    "\n",
    "    os.makedirs(QUANTIZED_MODEL_PRED_DIR, exist_ok=True)\n",
    "    os.makedirs(ORIGINAL_MODEL_PRED_DIR, exist_ok=True) # Ensure it exists\n",
    "    os.makedirs(GT_DIR, exist_ok=True)\n",
    "\n",
    "    # A. Evaluate Pre-Computed \"Original Model\" Predictions\n",
    "    image_paths = glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\"))\n",
    "    results_original = evaluate_csv_predictions(\n",
    "        image_paths=image_paths,\n",
    "        gt_dir=GT_DIR,\n",
    "        prediction_dir=ORIGINAL_MODEL_PRED_DIR,\n",
    "        class_names_map=CLASS_NAMES\n",
    "    )\n",
    "\n",
    "    # B. Evaluate Quantized Model (Live Inference)\n",
    "    print(\"\\n--- Evaluating QUANTIZED ESP-DL Model (Live Inference) ---\")\n",
    "\n",
    "    # Collect predictions from the live model\n",
    "    live_predictions = defaultdict(list)\n",
    "    live_ground_truths = defaultdict(list)\n",
    "\n",
    "    executor = TorchExecutor(graph=quantized_model, device=DEVICE)\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        image_name = os.path.basename(image_path)\n",
    "        image_base_name = os.path.splitext(image_name)[0]\n",
    "        gt_csv_path = os.path.join(GT_DIR, f\"output_{image_base_name}.csv\")\n",
    "        gt_boxes, gt_classes = load_ground_truth_from_csv(gt_csv_path)\n",
    "        for box, cls_id in zip(gt_boxes, gt_classes):\n",
    "            live_ground_truths[cls_id.item()].append([i, box.tolist()])\n",
    "\n",
    "\n",
    "        input_tensor, orig_shape, scales = preprocess_for_esp_dl(image_path, MODEL_INPUT_SHAPE, MODEL_MEAN, MODEL_STD)\n",
    "        input_tensor = input_tensor.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        outputs = executor(input_tensor)\n",
    "\n",
    "        final_results = postprocess_for_esp_dl(\n",
    "            outputs,\n",
    "            orig_shape,\n",
    "            scales,\n",
    "            conf_threshold=CONF_THRESHOLD,\n",
    "            iou_threshold=IOU_THRESHOLD,\n",
    "            max_detections=MAX_DETECTIONS\n",
    "        )\n",
    "\n",
    "        detections_for_saving = []\n",
    "        if final_results:\n",
    "            for detection_tuple in final_results:\n",
    "                class_id, score, x1, y1, x2, y2 = detection_tuple\n",
    "\n",
    "                # 1. Collect for metric calculation\n",
    "                box = [x1, y1, x2, y2]\n",
    "                live_predictions[int(class_id)].append([i, box, score])\n",
    "\n",
    "                # 2. Collect for saving, ensuring order is [x1, y1, x2, y2, score, class_id]\n",
    "                detections_for_saving.append([x1, y1, x2, y2, score, class_id])\n",
    "\n",
    "        # Convert to tensor for the save function\n",
    "        detections_tensor = torch.tensor(detections_for_saving, dtype=torch.float32)\n",
    "\n",
    "        # Define output path and save\n",
    "        quantized_pred_csv_path = os.path.join(QUANTIZED_MODEL_PRED_DIR, f\"output_{image_base_name}.csv\")\n",
    "        save_predictions_to_csv(detections_tensor, CLASS_NAMES, quantized_pred_csv_path)\n",
    "        print(f\"  - Saved {detections_tensor.shape[0]} predictions to '{quantized_pred_csv_path}'\")\n",
    "\n",
    "    results_quantized = calculate_metrics_from_collected_data(live_predictions, live_ground_truths)\n",
    "\n",
    "\n",
    "    # --- 3. Present Comparison Table ---\n",
    "    print(\"\\n\\n--- COMPREHENSIVE EVALUATION RESULTS ---\\n\")\n",
    "    print(\"Note: True Negatives (TN) are not reported as they are ill-defined for object detection tasks.\\n\")\n",
    "    header = f\"{'CLASS':<15} | {'METRIC':<18} | {'ORIGINAL MODEL':<16} | {'QUANTIZED MODEL':<17} | {'CHANGE':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    all_class_ids = sorted(list(set(results_original[\"ap_per_class\"].keys()) | set(results_quantized[\"ap_per_class\"].keys())))\n",
    "\n",
    "    for class_id in all_class_ids:\n",
    "        class_name = CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n",
    "        print(f\"{class_name:<15} | {'-'*18} | {'-'*16} | {'-'*17} | {'-'*10}\")\n",
    "\n",
    "        # AP\n",
    "        fp32_ap = results_original[\"ap_per_class\"].get(class_id, 0)\n",
    "        quant_ap = results_quantized[\"ap_per_class\"].get(class_id, 0)\n",
    "        ap_change = quant_ap - fp32_ap\n",
    "        print(f\"{'':<15} | {'AP @50':<18} | {fp32_ap:<16.4f} | {quant_ap:<17.4f} | {ap_change:<+10.4f}\")\n",
    "\n",
    "        # Avg IoU\n",
    "        fp32_iou = results_original[\"avg_iou_per_class\"].get(class_id, 0)\n",
    "        quant_iou = results_quantized[\"avg_iou_per_class\"].get(class_id, 0)\n",
    "        iou_change = quant_iou - fp32_iou\n",
    "        print(f\"{'':<15} | {'Avg IoU (TPs)':<18} | {fp32_iou:<16.4f} | {quant_iou:<17.4f} | {iou_change:<+10.4f}\")\n",
    "\n",
    "        # TP Count\n",
    "        fp32_tps = results_original[\"tps_per_class\"].get(class_id, 0)\n",
    "        quant_tps = results_quantized[\"tps_per_class\"].get(class_id, 0)\n",
    "        tps_change = quant_tps - fp32_tps\n",
    "        print(f\"{'':<15} | {'True Positives (TP)':<18} | {fp32_tps:<16} | {quant_tps:<17} | {tps_change:<+10}\")\n",
    "\n",
    "        # FP Count\n",
    "        fp32_fps = results_original[\"fps_per_class\"].get(class_id, 0)\n",
    "        quant_fps = results_quantized[\"fps_per_class\"].get(class_id, 0)\n",
    "        fps_change = quant_fps - fp32_fps\n",
    "        print(f\"{'':<15} | {'False Positives (FP)':<18} | {fp32_fps:<16} | {quant_fps:<17} | {fps_change:<+10}\")\n",
    "\n",
    "        # FN Count\n",
    "        fp32_fns = results_original[\"fns_per_class\"].get(class_id, 0)\n",
    "        quant_fns = results_quantized[\"fns_per_class\"].get(class_id, 0)\n",
    "        fns_change = quant_fns - fp32_fns\n",
    "        print(f\"{'':<15} | {'False Negatives (FN)':<18} | {fp32_fns:<16} | {quant_fns:<17} | {fns_change:<+10}\")\n",
    "\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    # Overall Metrics\n",
    "    print(f\"{'OVERALL':<15} | {'-'*18} | {'-'*16} | {'-'*17} | {'-'*10}\")\n",
    "    fp32_map = results_original.get(\"mAP\", 0)\n",
    "    quant_map = results_quantized.get(\"mAP\", 0)\n",
    "    map_change = quant_map - fp32_map\n",
    "    print(f\"{'':<15} | {'mAP @50':<18} | {fp32_map:<16.4f} | {quant_map:<17.4f} | {map_change:<+10.4f}\")\n",
    "\n",
    "    # Total TP/FP/FN\n",
    "    fp32_total_tps = sum(results_original.get(\"tps_per_class\", {}).values())\n",
    "    quant_total_tps = sum(results_quantized.get(\"tps_per_class\", {}).values())\n",
    "    total_tps_change = quant_total_tps - fp32_total_tps\n",
    "    print(f\"{'':<15} | {'Total TPs':<18} | {fp32_total_tps:<16} | {quant_total_tps:<17} | {total_tps_change:<+10}\")\n",
    "\n",
    "    fp32_total_fps = sum(results_original.get(\"fps_per_class\", {}).values())\n",
    "    quant_total_fps = sum(results_quantized.get(\"fps_per_class\", {}).values())\n",
    "    total_fps_change = quant_total_fps - fp32_total_fps\n",
    "    print(f\"{'':<15} | {'Total FPs':<18} | {fp32_total_fps:<16} | {quant_total_fps:<17} | {total_fps_change:<+10}\")\n",
    "\n",
    "    fp32_total_fns = sum(results_original.get(\"fns_per_class\", {}).values())\n",
    "    quant_total_fns = sum(results_quantized.get(\"fns_per_class\", {}).values())\n",
    "    total_fns_change = quant_total_fns - fp32_total_fns\n",
    "    print(f\"{'':<15} | {'Total FNs':<18} | {fp32_total_fns:<16} | {quant_total_fns:<17} | {total_fns_change:<+10}\")\n",
    "    print(\"-\" * len(header))\n",
    "    print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "20fd03b89b2a2b84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "\n",
      "Evaluating pre-computed CSVs from './preds_calib_images'...\n",
      "\n",
      "--- Evaluating QUANTIZED ESP-DL Model (Live Inference) ---\n",
      "  - Saved 15 predictions to './preds_quantized_model/output_frame85.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame52.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame53.csv'\n",
      "  - Saved 4 predictions to './preds_quantized_model/output_frame84.csv'\n",
      "  - Saved 14 predictions to './preds_quantized_model/output_frame86.csv'\n",
      "  - Saved 10 predictions to './preds_quantized_model/output_frame51.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame45.csv'\n",
      "  - Saved 11 predictions to './preds_quantized_model/output_frame44.csv'\n",
      "  - Saved 13 predictions to './preds_quantized_model/output_frame50.csv'\n",
      "  - Saved 7 predictions to './preds_quantized_model/output_frame87.csv'\n",
      "  - Saved 11 predictions to './preds_quantized_model/output_frame93.csv'\n",
      "  - Saved 10 predictions to './preds_quantized_model/output_frame83.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame54.csv'\n",
      "  - Saved 11 predictions to './preds_quantized_model/output_frame40.csv'\n",
      "  - Saved 6 predictions to './preds_quantized_model/output_frame68.csv'\n",
      "  - Saved 5 predictions to './preds_quantized_model/output_frame69.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame41.csv'\n",
      "  - Saved 2 predictions to './preds_quantized_model/output_frame55.csv'\n",
      "  - Saved 10 predictions to './preds_quantized_model/output_frame82.csv'\n",
      "  - Saved 1 predictions to './preds_quantized_model/output_frame43.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame42.csv'\n",
      "  - Saved 11 predictions to './preds_quantized_model/output_frame19.csv'\n",
      "  - Saved 18 predictions to './preds_quantized_model/output_frame25.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame31.csv'\n",
      "  - Saved 6 predictions to './preds_quantized_model/output_frame30.csv'\n",
      "  - Saved 17 predictions to './preds_quantized_model/output_frame24.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame18.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame32.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame26.csv'\n",
      "  - Saved 7 predictions to './preds_quantized_model/output_frame27.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame33.csv'\n",
      "  - Saved 5 predictions to './preds_quantized_model/output_frame37.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame23.csv'\n",
      "  - Saved 14 predictions to './preds_quantized_model/output_frame22.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame36.csv'\n",
      "  - Saved 15 predictions to './preds_quantized_model/output_frame20.csv'\n",
      "  - Saved 5 predictions to './preds_quantized_model/output_frame34.csv'\n",
      "  - Saved 16 predictions to './preds_quantized_model/output_frame9.csv'\n",
      "  - Saved 16 predictions to './preds_quantized_model/output_frame8.csv'\n",
      "  - Saved 6 predictions to './preds_quantized_model/output_frame35.csv'\n",
      "  - Saved 15 predictions to './preds_quantized_model/output_frame21.csv'\n",
      "  - Saved 7 predictions to './preds_quantized_model/output_frame38.csv'\n",
      "  - Saved 10 predictions to './preds_quantized_model/output_frame5.csv'\n",
      "  - Saved 15 predictions to './preds_quantized_model/output_frame10.csv'\n",
      "  - Saved 14 predictions to './preds_quantized_model/output_frame11.csv'\n",
      "  - Saved 6 predictions to './preds_quantized_model/output_frame4.csv'\n",
      "  - Saved 15 predictions to './preds_quantized_model/output_frame39.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame6.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame13.csv'\n",
      "  - Saved 18 predictions to './preds_quantized_model/output_frame12.csv'\n",
      "  - Saved 13 predictions to './preds_quantized_model/output_frame7.csv'\n",
      "  - Saved 11 predictions to './preds_quantized_model/output_frame3.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame16.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame17.csv'\n",
      "  - Saved 12 predictions to './preds_quantized_model/output_frame2.csv'\n",
      "  - Saved 4 predictions to './preds_quantized_model/output_frame0.csv'\n",
      "  - Saved 11 predictions to './preds_quantized_model/output_frame15.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame29.csv'\n",
      "  - Saved 4 predictions to './preds_quantized_model/output_frame28.csv'\n",
      "  - Saved 13 predictions to './preds_quantized_model/output_frame14.csv'\n",
      "  - Saved 11 predictions to './preds_quantized_model/output_frame1.csv'\n",
      "  - Saved 6 predictions to './preds_quantized_model/output_frame67.csv'\n",
      "  - Saved 6 predictions to './preds_quantized_model/output_frame73.csv'\n",
      "  - Saved 10 predictions to './preds_quantized_model/output_frame72.csv'\n",
      "  - Saved 8 predictions to './preds_quantized_model/output_frame66.csv'\n",
      "  - Saved 13 predictions to './preds_quantized_model/output_frame70.csv'\n",
      "  - Saved 12 predictions to './preds_quantized_model/output_frame71.csv'\n",
      "  - Saved 7 predictions to './preds_quantized_model/output_frame75.csv'\n",
      "  - Saved 13 predictions to './preds_quantized_model/output_frame49.csv'\n",
      "  - Saved 14 predictions to './preds_quantized_model/output_frame60.csv'\n",
      "  - Saved 9 predictions to './preds_quantized_model/output_frame74.csv'\n",
      "\n",
      "\n",
      "--- COMPREHENSIVE EVALUATION RESULTS ---\n",
      "\n",
      "Note: True Negatives (TN) are not reported as they are ill-defined for object detection tasks.\n",
      "\n",
      "CLASS           | METRIC             | ORIGINAL MODEL   | QUANTIZED MODEL   | CHANGE    \n",
      "----------------------------------------------------------------------------------------\n",
      "bicycle         | ------------------ | ---------------- | ----------------- | ----------\n",
      "                | AP @50             | 0.9307           | 0.6412            | -0.2895   \n",
      "                | Avg IoU (TPs)      | 0.8364           | 0.7661            | -0.0703   \n",
      "                | True Positives (TP) | 530              | 380               | -150      \n",
      "                | False Positives (FP) | 352              | 236               | -116      \n",
      "                | False Negatives (FN) | 18               | 168               | +150      \n",
      "saddle          | ------------------ | ---------------- | ----------------- | ----------\n",
      "                | AP @50             | 0.5684           | 0.1780            | -0.3904   \n",
      "                | Avg IoU (TPs)      | 0.7299           | 0.7259            | -0.0040   \n",
      "                | True Positives (TP) | 181              | 60                | -121      \n",
      "                | False Positives (FP) | 80               | 19                | -61       \n",
      "                | False Negatives (FN) | 122              | 243               | +121      \n",
      "----------------------------------------------------------------------------------------\n",
      "OVERALL         | ------------------ | ---------------- | ----------------- | ----------\n",
      "                | mAP @50            | 0.7496           | 0.4096            | -0.3400   \n",
      "                | Total TPs          | 711              | 440               | -271      \n",
      "                | Total FPs          | 432              | 255               | -177      \n",
      "                | Total FNs          | 140              | 411               | +271      \n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper Functions",
   "id": "593af22703afd9e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compress images to 640x640",
   "id": "adcf31c6db312350"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:48:10.978342Z",
     "start_time": "2025-07-06T10:48:09.381980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = \"./calib_images\"\n",
    "output_dir = \"./calib_images_compressed\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all JPG files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith('.jpg'):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Open, resize, and save the image\n",
    "        with Image.open(input_path) as img:\n",
    "            img_resized = img.resize((640, 640), Image.Resampling.LANCZOS)\n",
    "            img_resized.save(output_path, quality=95)\n",
    "            # Optional: use optimize flag for better compression\n",
    "            # img_resized.save(output_path, optimize=True, quality=95)\n",
    "\n",
    "print(\"Compression complete.\")\n"
   ],
   "id": "28646c755d6e65ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression complete.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize predictions on image",
   "id": "cb0ece1c7970df20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T16:11:22.749133Z",
     "start_time": "2025-07-06T16:11:22.737419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "# Load image\n",
    "image_path = \"./yolo11_detect/main/bikes.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "predictions = [\n",
    "    (0, 0.6926, 135, 363, 193, 462),\n",
    "    (0, 0.6225, 84,  356, 140, 454),\n",
    "    (0, 0.5000, 165, 352, 232, 489),\n",
    "    (0, 0.4533, 128, 362, 175, 460),\n",
    "    (0, 0.4378, 42,  348, 82,  426),\n",
    "    (0, 0.3923, 103, 361, 160, 458),\n",
    "    (0, 0.2814, 407, 364, 468, 490),\n",
    "    (0, 0.2689, 67,  354, 133, 444),\n",
    "    (0, 0.2568, 517, 185, 572, 296),\n",
    "    #(0, 0.2337, 112, 362, 172, 460), # This prediction is not on the esp\n",
    "    (0, 0.1480, 57,  351, 111, 434),\n",
    "    (0, 0.1480, 411, 357, 478, 479),\n",
    "    #(0, 0.1403, 21,  347, 62,  415), # This prediction is not on the esp\n",
    "    (0, 0.1128, 15,  345, 53,  417),\n",
    "\n",
    "]\n",
    "'''\n",
    "\n",
    "predictions = [\n",
    "    (0, 0.679179, 135, 362, 193, 462),\n",
    "    (0, 0.622459, 84,  356, 138, 450),\n",
    "    (0, 0.484380, 43,  348, 83,  426),\n",
    "    (0, 0.468791, 165, 352, 232, 491),\n",
    "    (0, 0.468791, 129, 362, 180, 460),\n",
    "    (0, 0.348645, 103, 360, 160, 457),\n",
    "    (0, 0.334589, 517, 185, 570, 296),\n",
    "    (0, 0.294215, 67,  354, 131, 442),\n",
    "    (0, 0.294215, 406, 365, 467, 492),\n",
    "    (0, 0.182426, 58,  350, 112, 434),\n",
    "    (0, 0.156105, 14,  346, 53,  416),\n",
    "    (0, 0.156105, 410, 357, 478, 480),\n",
    "]\n",
    "'''\n",
    "# Define colors for each class\n",
    "colors = {\n",
    "    0: (255, 0, 0),       # Blue for bicycles (BGR)\n",
    "    1: (255, 255, 51),   # Light blue for saddles\n",
    "}\n",
    "\n",
    "# Class names\n",
    "class_names = {\n",
    "    0: \"Bicycle\",\n",
    "    1: \"Saddle\",\n",
    "}\n",
    "\n",
    "# Loop through predictions and draw boxes\n",
    "for category, score, x1, y1, x2, y2 in predictions:\n",
    "    label = f\"{class_names[category]}: {score:.2f}\"\n",
    "    color = colors[category]\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "    cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"output_QAT_pc.jpg\", image)\n"
   ],
   "id": "4c85effb9a8a1722",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transform original labels to compressed labels",
   "id": "9d6fe7fc79e333d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T10:43:53.019459Z",
     "start_time": "2025-07-06T10:43:52.415011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Helper Functions (from previous answers) ---\n",
    "\n",
    "def yolo_to_xyxy(yolo_boxes, original_shape):\n",
    "    \"\"\"Converts YOLO format boxes to [x1, y1, x2, y2] format.\"\"\"\n",
    "    orig_h, orig_w = original_shape\n",
    "    yolo_boxes[:, 0] *= orig_w\n",
    "    yolo_boxes[:, 1] *= orig_h\n",
    "    yolo_boxes[:, 2] *= orig_w\n",
    "    yolo_boxes[:, 3] *= orig_h\n",
    "    xyxy_boxes = np.zeros_like(yolo_boxes)\n",
    "    xyxy_boxes[:, 0] = yolo_boxes[:, 0] - yolo_boxes[:, 2] / 2\n",
    "    xyxy_boxes[:, 1] = yolo_boxes[:, 1] - yolo_boxes[:, 3] / 2\n",
    "    xyxy_boxes[:, 2] = yolo_boxes[:, 0] + yolo_boxes[:, 2] / 2\n",
    "    xyxy_boxes[:, 3] = yolo_boxes[:, 1] + yolo_boxes[:, 3] / 2\n",
    "    return xyxy_boxes\n",
    "\n",
    "def transform_boxes_to_letterbox(boxes_xyxy, original_shape, new_shape=(640, 640)):\n",
    "    \"\"\"Transforms [x1, y1, x2, y2] boxes to letterboxed coordinates.\"\"\"\n",
    "    orig_h, orig_w = original_shape\n",
    "    new_h, new_w = new_shape\n",
    "    r = min(new_h / orig_h, new_w / orig_w)\n",
    "    new_unpad_w, new_unpad_h = int(round(orig_w * r)), int(round(orig_h * r))\n",
    "    dw = (new_w - new_unpad_w) / 2\n",
    "    dh = (new_h - new_unpad_h) / 2\n",
    "    boxes = np.array(boxes_xyxy, dtype=np.float32)\n",
    "    boxes[:, [0, 2]] = boxes[:, [0, 2]] * r + dw\n",
    "    boxes[:, [1, 3]] = boxes[:, [1, 3]] * r + dh\n",
    "    return boxes\n",
    "\n",
    "def save_labels_to_csv(boxes, class_ids, class_names_map, output_csv_path):\n",
    "    \"\"\"Saves transformed labels to a CSV file.\"\"\"\n",
    "    columns = ['name', 'class', 'confidence', 'box']\n",
    "    rows = []\n",
    "\n",
    "    # Note: Ground truth has no 'confidence', so we can use a placeholder like 1.0\n",
    "    for i, box in enumerate(boxes):\n",
    "        class_id = class_ids[i]\n",
    "        class_name = class_names_map.get(class_id, f\"class_{class_id}\")\n",
    "        box_string = f\"{{'x1': {box[0]}, 'y1': {box[1]}, 'x2': {box[2]}, 'y2': {box[3]}}}\"\n",
    "        rows.append({\n",
    "            'name': class_name,\n",
    "            'class': class_id,\n",
    "            'confidence': 1.0,\n",
    "            'box': box_string\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# --- Main Conversion Script ---\n",
    "\n",
    "def convert_labels():\n",
    "    # --- Configuration ---\n",
    "    # Path to original, high-resolution images\n",
    "    ORIGINAL_IMAGE_DIR = \"/Users/christophknaden/git/leezencounter/model-training/datasets/security_camera/YOLO/images\"\n",
    "    # Path to original YOLO .txt labels\n",
    "    ORIGINAL_LABEL_DIR = \"/Users/christophknaden/git/leezencounter/model-training/datasets/security_camera/YOLO/labels\"\n",
    "    # Path where you want to save the new ground truth CSVs\n",
    "    NEW_CSV_DIR = \"./new_ground_truth_labels\"\n",
    "\n",
    "    # !!! IMPORTANT: Update this to match your classes.txt or data.yaml !!!\n",
    "    CLASS_NAMES = {\n",
    "        0: 'bicycle',\n",
    "        1: 'saddle'\n",
    "        # Add all your classes here\n",
    "    }\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(NEW_CSV_DIR, exist_ok=True)\n",
    "    print(f\"New CSV labels will be saved to: {os.path.abspath(NEW_CSV_DIR)}\")\n",
    "\n",
    "    # Find all original label files\n",
    "    label_files = glob.glob(os.path.join(ORIGINAL_LABEL_DIR, \"*.txt\"))\n",
    "\n",
    "    if not label_files:\n",
    "        print(f\"Error: No label files found in '{ORIGINAL_LABEL_DIR}'. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(label_files)} label files to process.\")\n",
    "\n",
    "    for label_path in label_files:\n",
    "        # --- 1. Load Original Data ---\n",
    "\n",
    "        # Load the YOLO .txt file\n",
    "        try:\n",
    "            data = np.loadtxt(label_path, ndmin=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read or empty file '{label_path}'. Skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        class_ids = data[:, 0].astype(int)\n",
    "        yolo_boxes = data[:, 1:]\n",
    "\n",
    "        # Find the corresponding original image to get its dimensions\n",
    "        base_name = os.path.splitext(os.path.basename(label_path))[0]\n",
    "        # Assume images can be .jpg, .png, etc.\n",
    "        image_path_pattern = os.path.join(ORIGINAL_IMAGE_DIR, f\"{base_name}.*\")\n",
    "        image_paths = glob.glob(image_path_pattern)\n",
    "\n",
    "        if not image_paths:\n",
    "            print(f\"Warning: No matching image found for label '{label_path}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        original_image = cv2.imread(image_paths[0])\n",
    "        original_shape = original_image.shape[:2] # (height, width)\n",
    "\n",
    "        # --- 2. Perform Transformations ---\n",
    "\n",
    "        # a. De-normalize YOLO boxes to pixel coordinates [x1, y1, x2, y2]\n",
    "        pixel_boxes = yolo_to_xyxy(yolo_boxes, original_shape)\n",
    "\n",
    "        # b. Transform pixel boxes to the 640x640 letterboxed space\n",
    "        transformed_boxes = transform_boxes_to_letterbox(pixel_boxes, original_shape)\n",
    "\n",
    "        # --- 3. Save the New CSV File ---\n",
    "\n",
    "        # The output filename should match the format your evaluation script expects\n",
    "        output_csv_name = f\"output_{base_name}.csv\"\n",
    "        output_csv_path = os.path.join(NEW_CSV_DIR, output_csv_name)\n",
    "\n",
    "        save_labels_to_csv(transformed_boxes, class_ids, CLASS_NAMES, output_csv_path)\n",
    "\n",
    "        print(f\"Successfully converted '{os.path.basename(label_path)}' -> '{output_csv_name}'\")\n",
    "\n",
    "# Run the conversion process\n",
    "if __name__ == \"__main__\":\n",
    "    convert_labels()"
   ],
   "id": "2d28e2cd41626870",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV labels will be saved to: /Users/christophknaden/git/leezencounter/model-deployment/new_ground_truth_labels\n",
      "Found 71 label files to process.\n",
      "Successfully converted 'frame30.txt' -> 'output_frame30.csv'\n",
      "Successfully converted 'frame24.txt' -> 'output_frame24.csv'\n",
      "Successfully converted 'frame18.txt' -> 'output_frame18.csv'\n",
      "Successfully converted 'frame19.txt' -> 'output_frame19.csv'\n",
      "Successfully converted 'frame25.txt' -> 'output_frame25.csv'\n",
      "Successfully converted 'frame31.txt' -> 'output_frame31.csv'\n",
      "Successfully converted 'frame27.txt' -> 'output_frame27.csv'\n",
      "Successfully converted 'frame33.txt' -> 'output_frame33.csv'\n",
      "Successfully converted 'frame32.txt' -> 'output_frame32.csv'\n",
      "Successfully converted 'frame26.txt' -> 'output_frame26.csv'\n",
      "Successfully converted 'frame22.txt' -> 'output_frame22.csv'\n",
      "Successfully converted 'frame36.txt' -> 'output_frame36.csv'\n",
      "Successfully converted 'frame37.txt' -> 'output_frame37.csv'\n",
      "Successfully converted 'frame23.txt' -> 'output_frame23.csv'\n",
      "Successfully converted 'frame8.txt' -> 'output_frame8.csv'\n",
      "Successfully converted 'frame35.txt' -> 'output_frame35.csv'\n",
      "Successfully converted 'frame21.txt' -> 'output_frame21.csv'\n",
      "Successfully converted 'frame20.txt' -> 'output_frame20.csv'\n",
      "Successfully converted 'frame34.txt' -> 'output_frame34.csv'\n",
      "Successfully converted 'frame9.txt' -> 'output_frame9.csv'\n",
      "Successfully converted 'frame84.txt' -> 'output_frame84.csv'\n",
      "Successfully converted 'frame53.txt' -> 'output_frame53.csv'\n",
      "Successfully converted 'frame52.txt' -> 'output_frame52.csv'\n",
      "Successfully converted 'frame85.txt' -> 'output_frame85.csv'\n",
      "Successfully converted 'frame87.txt' -> 'output_frame87.csv'\n",
      "Successfully converted 'frame93.txt' -> 'output_frame93.csv'\n",
      "Successfully converted 'frame44.txt' -> 'output_frame44.csv'\n",
      "Successfully converted 'frame50.txt' -> 'output_frame50.csv'\n",
      "Successfully converted 'frame51.txt' -> 'output_frame51.csv'\n",
      "Successfully converted 'frame45.txt' -> 'output_frame45.csv'\n",
      "Successfully converted 'frame86.txt' -> 'output_frame86.csv'\n",
      "Successfully converted 'frame82.txt' -> 'output_frame82.csv'\n",
      "Successfully converted 'frame69.txt' -> 'output_frame69.csv'\n",
      "Successfully converted 'frame41.txt' -> 'output_frame41.csv'\n",
      "Successfully converted 'frame55.txt' -> 'output_frame55.csv'\n",
      "Successfully converted 'frame54.txt' -> 'output_frame54.csv'\n",
      "Successfully converted 'frame40.txt' -> 'output_frame40.csv'\n",
      "Successfully converted 'frame68.txt' -> 'output_frame68.csv'\n",
      "Successfully converted 'frame83.txt' -> 'output_frame83.csv'\n",
      "Successfully converted 'frame42.txt' -> 'output_frame42.csv'\n",
      "Successfully converted 'frame43.txt' -> 'output_frame43.csv'\n",
      "Successfully converted 'frame72.txt' -> 'output_frame72.csv'\n",
      "Successfully converted 'frame66.txt' -> 'output_frame66.csv'\n",
      "Successfully converted 'frame67.txt' -> 'output_frame67.csv'\n",
      "Successfully converted 'frame73.txt' -> 'output_frame73.csv'\n",
      "Successfully converted 'frame71.txt' -> 'output_frame71.csv'\n",
      "Successfully converted 'frame70.txt' -> 'output_frame70.csv'\n",
      "Successfully converted 'frame60.txt' -> 'output_frame60.csv'\n",
      "Successfully converted 'frame74.txt' -> 'output_frame74.csv'\n",
      "Successfully converted 'frame75.txt' -> 'output_frame75.csv'\n",
      "Successfully converted 'frame49.txt' -> 'output_frame49.csv'\n",
      "Successfully converted 'frame4.txt' -> 'output_frame4.csv'\n",
      "Successfully converted 'frame11.txt' -> 'output_frame11.csv'\n",
      "Successfully converted 'frame39.txt' -> 'output_frame39.csv'\n",
      "Successfully converted 'frame38.txt' -> 'output_frame38.csv'\n",
      "Successfully converted 'frame10.txt' -> 'output_frame10.csv'\n",
      "Successfully converted 'frame5.txt' -> 'output_frame5.csv'\n",
      "Successfully converted 'frame7.txt' -> 'output_frame7.csv'\n",
      "Successfully converted 'frame12.txt' -> 'output_frame12.csv'\n",
      "Successfully converted 'frame13.txt' -> 'output_frame13.csv'\n",
      "Successfully converted 'frame6.txt' -> 'output_frame6.csv'\n",
      "Successfully converted 'frame2.txt' -> 'output_frame2.csv'\n",
      "Successfully converted 'frame17.txt' -> 'output_frame17.csv'\n",
      "Successfully converted 'frame16.txt' -> 'output_frame16.csv'\n",
      "Successfully converted 'frame3.txt' -> 'output_frame3.csv'\n",
      "Successfully converted 'frame28.txt' -> 'output_frame28.csv'\n",
      "Successfully converted 'frame1.txt' -> 'output_frame1.csv'\n",
      "Successfully converted 'frame14.txt' -> 'output_frame14.csv'\n",
      "Successfully converted 'frame15.txt' -> 'output_frame15.csv'\n",
      "Successfully converted 'frame0.txt' -> 'output_frame0.csv'\n",
      "Successfully converted 'frame29.txt' -> 'output_frame29.csv'\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5dca025805fa9cd3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
